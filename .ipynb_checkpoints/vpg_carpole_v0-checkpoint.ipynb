{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-rc3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews, dtype='float32')\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "        \n",
    "    return rtgs\n",
    "\n",
    "def random_sample(logits):\n",
    "    return tf.random.categorical(logits, 1)\n",
    "\n",
    "def vstack(arr):\n",
    "    return tf.squeeze(tf.stack(arr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(tf.keras.Model):\n",
    "    '''tensorflow model with linear activation on output level'''\n",
    "\n",
    "    def __init__(self, units, activations):\n",
    "        assert len(units) == len(activations) + 2\n",
    "        super().__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.activations = activations\n",
    "        self.build_graph()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.graph = [tf.keras.layers.Dense(units, activation=activation)\n",
    "                       for (units, activation) in zip(self.units[1:], self.activations + ['linear'])\n",
    "                      ]\n",
    "        \n",
    "    def call(self, x):\n",
    "        h = x\n",
    "        for layer in self.graph:\n",
    "            h = layer(h)\n",
    "        return h\n",
    "    \n",
    "class VPG(object):\n",
    "    '''Vanilla Policy Gradient'''\n",
    "    def __init__(self, env, model, optimizer, render=False):\n",
    "        assert isinstance(env.observation_space, gym.spaces.Box), \\\n",
    "            \"This example only works for envs with continuous state spaces.\"\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete), \\\n",
    "            \"This example only works for envs with discrete action spaces.\"\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.render = render\n",
    "        \n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.n_acts = env.action_space.n\n",
    "        \n",
    "    def loss_object(self, actions, logits, weights):\n",
    "        log_probs = tf.reduce_sum(tf.one_hot(actions, self.n_acts) * tf.nn.log_softmax(logits),\n",
    "                                  axis=1\n",
    "                                 )\n",
    "        return -tf.reduce_mean(weights * log_probs)\n",
    "    \n",
    "    def train_one_epoch(self, batch_size):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # make some empty lists for logging.\n",
    "            batch_logits = []          # for observations\n",
    "            batch_acts = []         # for actions\n",
    "            batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "            batch_rets = []         # for measuring episode returns\n",
    "            batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "            # reset episode-specific variables\n",
    "            obs = self.env.reset()       # first obs comes from starting distribution\n",
    "            done = False            # signal from environment that episode is over\n",
    "            ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "            # render first episode of each epoch\n",
    "            finished_rendering_this_epoch = False\n",
    "\n",
    "            # collect experience by acting in the environment with current policy\n",
    "            while True:\n",
    "                # rendering\n",
    "                if (not finished_rendering_this_epoch) and self.render:\n",
    "                    env.render()\n",
    "\n",
    "                # act in the environment\n",
    "                logits = self.model(obs.reshape(1,-1), training=True)\n",
    "                act = random_sample(logits)[0, 0].numpy()\n",
    "                obs, rew, done, _ = self.env.step(act)\n",
    "\n",
    "                # save action, reward\n",
    "                batch_logits.append(logits)\n",
    "                batch_acts.append(act)\n",
    "                ep_rews.append(rew)\n",
    "\n",
    "                if done:\n",
    "                    # if episode is over, record info about episode\n",
    "                    ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                    batch_rets.append(ep_ret)\n",
    "                    batch_lens.append(ep_len)\n",
    "\n",
    "                    # the weight for each logprob(a|s) is R(tau)\n",
    "                    batch_weights += list(reward_to_go(ep_rews))\n",
    "\n",
    "                    # reset episode-specific variables\n",
    "                    obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                    # won't render again this epoch\n",
    "                    finished_rendering_this_epoch = True\n",
    "\n",
    "                    # end experience loop if we have enough of it\n",
    "                    if len(batch_logits) > batch_size:\n",
    "                        break\n",
    "\n",
    "            loss = self.loss_object(batch_acts, vstack(batch_logits), tf.stack(batch_weights))\n",
    "\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        return loss, batch_rets, batch_lens\n",
    "\n",
    "    def train(self, epochs=50, batch_size=5000, verbose=1):\n",
    "        for i in range(epochs):\n",
    "            batch_loss, batch_rets, batch_lens = self.train_one_epoch(batch_size)\n",
    "            if verbose > 0:\n",
    "                print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                        (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
    "                \n",
    "    def act(self, observation):\n",
    "        logits = self.model(observation.reshape(1,-1), training=False)\n",
    "        return random_sample(logits)[0, 0].numpy()\n",
    "    \n",
    "    def demo(self, n_steps=200):\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            env.render()\n",
    "            action = self.act(observation)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "            if done:\n",
    "                observation = self.env.reset()\n",
    "                time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net([env.observation_space.shape[0], 32, env.action_space.n], ['tanh'])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer net_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "epoch:   0 \t loss: -10.118 \t return: 22.044 \t ep_len: 22.044\n",
      "epoch:   1 \t loss: -8.190 \t return: 18.266 \t ep_len: 18.266\n"
     ]
    }
   ],
   "source": [
    "vpg = VPG(env, model, optimizer)\n",
    "vpg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf2venv",
   "language": "python",
   "name": ".tf2venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
